#Number Check Scala Program  
 object NumberCheck {
  def main(args: Array[String]): Unit = {
    println("Enter a number:")
    val number = scala.io.StdIn.readInt()

    if (number > 0) {
      println("The number is positive.")
    } else if (number < 0) {
      println("The number is negative.")
    } else {
      println("The number is zero.")
    }
  }
}

# Largest Number Check 

object LargestOfTwoNumbers {
  def main(args: Array[String]): Unit = {
    println("Enter the first number:")
    val number1 = scala.io.StdIn.readInt()

    println("Enter the second number:")
    val number2 = scala.io.StdIn.readInt()
    
    if(number1 == number2){
        println(s"Both Numbers are same: $number2")
    } else{
      val largest = if (number1 > number2) number1 else number2
      val smallest = if (number1 < number2) number1 else number2

      println(s"The largest number is: $largest")
      println(s"The smallest number is: $smallest")
    }

    
  }
}

/*
    Problem Statement No. 05
Write a Scala Program to process a log file of a system and perform following analytics on the given dataset.
(I) Display the list of top 10 frequent hosts.
(II) Display the list of top 5 URLs or paths
(III) Display the number of unique Hosts

*/

val pathToDataSet = "/home/kali/DJABRJ/spark_pract/weblog.csv"

val df = spark.read.option("header", "true").option("inferSchema", "true")csv(pathToDataSet)


val topHosts = df.groupBy("IP").count().orderBy($"count".desc).limit(10)
val topUrls = df.groupBy("URL").count().orderBy($"count".desc).limit(5)
val uniqueHosts = df.select("IP").distinct().count()



/*
Write a Scala Program to process a log file of a system and perform following analytics on the given dataset.
(I) Display the count of 404 Response Codes 
(II) Display the list of Top Twenty-five 404 Response Code Hosts
(III) Display the number of Unique Daily Hosts

*/

val pathToDataSet = "/home/kali/DJABRJ/spark_pract/weblog.csv"

val df = spark.read.option("header", "true").option("inferSchema", "true")csv(pathToDataSet)

val cleaned_df = df.filter($"IP".rlike("^\\d+\\.\\d+\\.\\d+\\.\\d+$"))


val count404 = cleaned_df.filter($"Status" === 404).count()


val top404Hosts = cleaned_df.filter($"Status" === 404).groupBy("IP").count().orderBy($"count".desc).limit(25)
println("Top Twenty-five 404 Response Code Hosts:")
top404Hosts.show(false)

val uniqueDailyHosts = cleaned_df.select(regexp_extract($"Time", """\d+/[A-Z][a-z]+/\d{4}""", 0).alias("Date"), $"IP").groupBy("Date").agg(countDistinct("IP").as("Distinct IP Count") )

println("Number of Unique Daily Hosts: ")
uniqueDailyHosts.show()




/* Below Not Required*/

val uniqueDailyHosts = cleaned_df.select($"IP", date_format($"Time", "yyyy-MM-dd").alias("Date")).distinct().groupBy("Date").count().count()
println("Number of Unique Daily Hosts: " + uniqueDailyHosts)


import org.apache.spark.sql.functions._

val formatted_df = cleaned_df.withColumn("FormattedTime", from_unixtime(unix_timestamp($"Time", "[dd/MMM/yyyy:HH:mm:ss")))
formatted_df.show(false)

spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

val formatted_df = cleaned_df.withColumn("FormattedTime", from_unixtime(unix_timestamp($"Time", "[dd/MMM/yyyy:HH:mm:ss")))
formatted_df.show(false)

val uniqueDailyHosts = formatted_df.select($"IP", date_format($"FormattedTime", "yyyy-MM-dd").alias("Date")).groupBy("Date").count()
val uniqueDailyHosts_count = uniqueDailyHosts.count().intValue()

println("Number of Unique Daily Hosts: ")
uniqueDailyHosts.show(uniqueDailyHosts_count)