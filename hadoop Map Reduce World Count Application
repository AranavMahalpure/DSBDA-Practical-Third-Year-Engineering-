Su - hadoop
password student
cd hadoop
hadoop version
nano file1.txt ( file creation and store)
start-all.sh / cd sbin           / cd bin           vs stop.sh
               start-dfs.sh       start- yarn.sh
hdfs mkdi/testwc
hdfs dfs -put (file moves in hadoop)
hdfs dfs -put/home/hadoop/file.txt /textwc
(alternative)  -put/admin://home/hadoop/hadoop/file.txt
// (localhost:9870   =>   Hadoop DFS)

// (make sure file name must b similar to class name )
nano WC_mapper.java
package com.wc;
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

public class WC_Mapper extends MapReduceBase implements
Mapper&lt;LongWritable,Text,Text,IntWritable&gt;
{
private final static IntWritable one = new IntWritable(1);
public void map(LongWritable key, Text value,OutputCollector&lt;Text,IntWritable&gt;
output,Reporter reporter) throws IOException
{
String line = value.toString();
StringTokenizer tokenizer = new StringTokenizer(line); \
while (tokenizer.hasMoreTokens())
{

word.set(tokenizer.nextToken());
output.collect(word, one);

}
}
}

WC_Reducer.java
package com.wc;

import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

public class WC_Reducer extends MapReduceBase implements
Reducer&lt;Text,IntWritable,Text,IntWritable&gt;
{
public void reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text,IntWritable&gt;
output, Reporter reporter) throws IOException
{
int sum=0;

while (values.hasNext())

{

sum+=values.next().get();
}

output.collect(key,new IntWritable(sum));
}
}

WC_Runner.java
package com.wc;

import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;

public class WC_Runner
{
public static void main(String[] args) throws IOException
{
JobConf conf = new JobConf(WC_Runner.class);
conf.setJobName(&quot;WordCount&quot;);
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
conf.setMapperClass(WC_Mapper.class);
conf.setCombinerClass(WC_Reducer.class);
conf.setReducerClass(WC_Reducer.class);
conf.setInputFormat(TextInputFormat.class);
conf.setOutputFormat(TextOutputFormat.class);

FileInputFormat.setInputPaths(conf,new Path(args[0]));
FileOutputFormat.setOutputPath(conf,new Path(args[1]));

JobClient.runJob(conf);
}
}

javac -classpath "$(hadoop classpath)" -d . WC_Mapper.java WC_Reducer.java WC_Runner.java
jar -cvf wordcount.jar com
hadoop jar /home/hadoop/hadoop/wordcount.jar com.WC.WC_Runner/test_wc/data1.txt /output
hdfs dfs -cat /r_output/part-00000
